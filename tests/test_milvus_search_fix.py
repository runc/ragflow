#!/usr/bin/env python3
"""
æµ‹è¯• Milvus æœç´¢ä¿®å¤çš„è„šæœ¬
"""

import os
import sys
import logging
import json

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DOC_ENGINE'] = 'milvus'

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'ragflow'))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_milvus_search_fix():
    """æµ‹è¯• Milvus æœç´¢ä¿®å¤"""
    try:
        logger.info("=== æµ‹è¯• Milvus æœç´¢ä¿®å¤ ===")
        
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from rag.utils.milvus_conn import MilvusConnection
        from rag.utils.doc_store_conn import MatchDenseExpr, OrderByExpr
        from pymilvus import utility
        
        # åˆ›å»ºè¿æ¥
        conn = MilvusConnection()
        logger.info("âœ“ Milvus è¿æ¥åˆ›å»ºæˆåŠŸ")
        
        # è·å–æ‰€æœ‰é›†åˆ
        collections = utility.list_collections(using=conn.alias)
        logger.info(f"âœ“ æ‰¾åˆ° {len(collections)} ä¸ªé›†åˆ")
        
        if not collections:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é›†åˆï¼Œæ— æ³•è¿›è¡Œæœç´¢æµ‹è¯•")
            return False
        
        # æ‰¾åˆ°æœ‰æ•°æ®çš„é›†åˆ
        test_collection_name = None
        for collection_name in collections:
            from pymilvus import Collection
            collection = Collection(name=collection_name, using=conn.alias)
            collection.load()
            if collection.num_entities > 0:
                test_collection_name = collection_name
                break
        
        if not test_collection_name:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•°æ®çš„é›†åˆï¼Œæ— æ³•è¿›è¡Œæœç´¢æµ‹è¯•")
            return False
        
        logger.info(f"ä½¿ç”¨æœ‰æ•°æ®çš„é›†åˆè¿›è¡Œæµ‹è¯•: {test_collection_name}")
        
        # è§£æé›†åˆåç§°
        parts = test_collection_name.split('_')
        if len(parts) < 2:
            logger.error(f"é›†åˆåç§°æ ¼å¼ä¸æ­£ç¡®: {test_collection_name}")
            return False
        
        index_name = '_'.join(parts[:-1])
        kb_id = parts[-1]
        logger.info(f"è§£æç»“æœ - ç´¢å¼•å: {index_name}, çŸ¥è¯†åº“ID: {kb_id}")
        
        # æ£€æŸ¥é›†åˆæ˜¯å¦æœ‰æ•°æ®
        from pymilvus import Collection
        collection = Collection(name=test_collection_name, using=conn.alias)
        collection.load()
        
        entity_count = collection.num_entities
        logger.info(f"é›†åˆ {test_collection_name} åŒ…å« {entity_count} ä¸ªå®ä½“")
        
        if entity_count == 0:
            logger.warning("é›†åˆä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæœç´¢æµ‹è¯•")
            return False
        
        # è·å–é›†åˆçš„å‘é‡å­—æ®µä¿¡æ¯
        from pymilvus import DataType
        vector_fields = []
        for field in collection.schema.fields:
            if field.dtype == DataType.FLOAT_VECTOR:
                vector_fields.append({
                    'name': field.name,
                    'dim': field.params.get('dim', 0) if hasattr(field, 'params') else 0
                })
        
        logger.info(f"æ‰¾åˆ°å‘é‡å­—æ®µ: {vector_fields}")
        
        if not vector_fields:
            logger.error("é›†åˆä¸­æ²¡æœ‰æ‰¾åˆ°å‘é‡å­—æ®µ")
            return False
        
        # æµ‹è¯•æ¯ä¸ªå‘é‡å­—æ®µ
        search_success = False
        for vector_field in vector_fields:
            field_name = vector_field['name']
            dim = vector_field['dim']
            
            if dim == 0:
                logger.warning(f"å‘é‡å­—æ®µ {field_name} ç»´åº¦æœªçŸ¥ï¼Œè·³è¿‡")
                continue
            
            logger.info(f"\næµ‹è¯•å‘é‡å­—æ®µ: {field_name} (ç»´åº¦: {dim})")
            
            try:
                # åˆ›å»ºæµ‹è¯•å‘é‡
                test_vector = [0.1 * i for i in range(dim)]
                
                # åˆ›å»ºåŒ¹é…è¡¨è¾¾å¼
                match_expr = MatchDenseExpr(
                    vector_column_name=field_name,
                    embedding_data=test_vector,
                    embedding_data_type="float",
                    distance_type="L2",
                    topn=10
                )
                
                # æ‰§è¡Œæœç´¢
                logger.info("æ‰§è¡Œæœç´¢...")
                results = conn.search(
                    selectFields=["id", "doc_id", "kb_id", "content_ltks"],
                    highlightFields=[],
                    condition={},
                    matchExprs=[match_expr],
                    orderBy=OrderByExpr(),
                    offset=0,
                    limit=5,
                    indexNames=[index_name],
                    knowledgebaseIds=[kb_id]
                )
                
                # æ£€æŸ¥ç»“æœ
                total = conn.getTotal(results)
                chunk_ids = conn.getChunkIds(results)
                
                logger.info(f"æœç´¢ç»“æœ:")
                logger.info(f"  - æ€»æ•°: {total}")
                logger.info(f"  - è¿”å›æ¡ç›®: {len(chunk_ids)}")
                logger.info(f"  - ç»“æœæ ¼å¼: {type(results)}")
                
                if total > 0:
                    logger.info(f"  - ç¤ºä¾‹ID: {chunk_ids[:3]}")
                    logger.info(f"  âœ“ æœç´¢æˆåŠŸ! æ‰¾åˆ° {total} æ¡ç»“æœ")
                    search_success = True
                    
                    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
                    if "hits" in results and "hits" in results["hits"]:
                        for i, hit in enumerate(results["hits"]["hits"][:3]):
                            logger.info(f"    ç»“æœ {i+1}:")
                            logger.info(f"      ID: {hit.get('_id', 'N/A')}")
                            logger.info(f"      Score: {hit.get('_score', 'N/A')}")
                            source = hit.get('_source', {})
                            logger.info(f"      Doc ID: {source.get('doc_id', 'N/A')}")
                            logger.info(f"      KB ID: {source.get('kb_id', 'N/A')}")
                            content = source.get('content_ltks', '')
                            if content and len(content) > 100:
                                content = content[:100] + "..."
                            logger.info(f"      Content: {content}")
                    
                    break
                else:
                    logger.warning(f"  æœç´¢æ— ç»“æœ")
                    
            except Exception as e:
                logger.error(f"  æœç´¢å¤±è´¥: {e}")
                import traceback
                logger.error(traceback.format_exc())
        
        if search_success:
            logger.info("\nâœ“ Milvus æœç´¢ä¿®å¤éªŒè¯æˆåŠŸ!")
            return True
        else:
            logger.error("\nâœ— Milvus æœç´¢ä¿®å¤éªŒè¯å¤±è´¥ - æ‰€æœ‰æœç´¢éƒ½æ— ç»“æœ")
            return False
            
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def test_pagination():
    """æµ‹è¯•åˆ†é¡µåŠŸèƒ½"""
    try:
        logger.info("\n=== æµ‹è¯•åˆ†é¡µåŠŸèƒ½ ===")
        
        from rag.utils.milvus_conn import MilvusConnection
        from rag.utils.doc_store_conn import MatchDenseExpr, OrderByExpr
        from pymilvus import utility
        
        conn = MilvusConnection()
        collections = utility.list_collections(using=conn.alias)
        
        if not collections:
            logger.warning("æ²¡æœ‰é›†åˆå¯ç”¨äºåˆ†é¡µæµ‹è¯•")
            return False
        
        # æ‰¾åˆ°æœ‰æ•°æ®çš„é›†åˆ
        test_collection_name = None
        for collection_name in collections:
            from pymilvus import Collection
            collection = Collection(name=collection_name, using=conn.alias)
            collection.load()
            if collection.num_entities > 0:
                test_collection_name = collection_name
                break
        
        if not test_collection_name:
            logger.warning("æ²¡æœ‰æœ‰æ•°æ®çš„é›†åˆå¯ç”¨äºåˆ†é¡µæµ‹è¯•")
            return False
        
        parts = test_collection_name.split('_')
        # å¯¹äº ragflow_ å¼€å¤´çš„é›†åˆï¼Œç‰¹æ®Šå¤„ç†
        if test_collection_name.startswith('ragflow_'):
            index_name = '_'.join(parts[:-1])
            kb_id = parts[-1]
        else:
            index_name = '_'.join(parts[:-1])
            kb_id = parts[-1]
        
        # è·å–å‘é‡å­—æ®µ
        from pymilvus import Collection
        collection = Collection(name=test_collection_name, using=conn.alias)
        collection.load()
        
        from pymilvus import DataType
        vector_field = None
        for field in collection.schema.fields:
            if field.dtype == DataType.FLOAT_VECTOR:
                vector_field = field
                break
        
        if not vector_field:
            logger.warning("æ²¡æœ‰å‘é‡å­—æ®µå¯ç”¨äºåˆ†é¡µæµ‹è¯•")
            return False
        
        dim = vector_field.params.get('dim', 1024) if hasattr(vector_field, 'params') else 1024
        test_vector = [0.1 * i for i in range(dim)]
        
        match_expr = MatchDenseExpr(
            vector_column_name=vector_field.name,
            embedding_data=test_vector,
            embedding_data_type="float",
            distance_type="L2",
            topn=20
        )
        
        # æµ‹è¯•ä¸åŒçš„åˆ†é¡µå‚æ•°
        page_tests = [
            {"offset": 0, "limit": 3},
            {"offset": 3, "limit": 3},
            {"offset": 6, "limit": 3},
        ]
        
        for test_params in page_tests:
            offset = test_params["offset"]
            limit = test_params["limit"]
            
            logger.info(f"æµ‹è¯•åˆ†é¡µ: offset={offset}, limit={limit}")
            
            results = conn.search(
                selectFields=["id", "doc_id"],
                highlightFields=[],
                condition={},
                matchExprs=[match_expr],
                orderBy=OrderByExpr(),
                offset=offset,
                limit=limit,
                indexNames=[index_name],
                knowledgebaseIds=[kb_id]
            )
            
            total = conn.getTotal(results)
            chunk_ids = conn.getChunkIds(results)
            
            logger.info(f"  ç»“æœ: æ€»æ•°={total}, è¿”å›={len(chunk_ids)}, IDs={chunk_ids}")
        
        logger.info("âœ“ åˆ†é¡µæµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"åˆ†é¡µæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹ Milvus æœç´¢ä¿®å¤éªŒè¯...")
    
    # æµ‹è¯•æœç´¢ä¿®å¤
    search_success = test_milvus_search_fix()
    
    # æµ‹è¯•åˆ†é¡µåŠŸèƒ½
    pagination_success = test_pagination()
    
    if search_success and pagination_success:
        logger.info("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! Milvus æœç´¢ä¿®å¤æˆåŠŸ!")
    else:
        logger.error("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    
    logger.info("æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main()
