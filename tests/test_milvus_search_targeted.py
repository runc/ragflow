#!/usr/bin/env python3
"""
é’ˆå¯¹æœ‰æ•°æ®é›†åˆçš„ Milvus æœç´¢æµ‹è¯•è„šæœ¬
"""

import os
import sys
import logging

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DOC_ENGINE'] = 'milvus'

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'ragflow'))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_with_data_collection():
    """æµ‹è¯•æœ‰æ•°æ®çš„é›†åˆ"""
    try:
        logger.info("=== æµ‹è¯•æœ‰æ•°æ®çš„é›†åˆ ===")
        
        from rag.utils.milvus_conn import MilvusConnection
        from rag.utils.doc_store_conn import MatchDenseExpr, OrderByExpr
        from pymilvus import utility, Collection
        
        # åˆ›å»ºè¿æ¥
        conn = MilvusConnection()
        
        # è·å–æ‰€æœ‰é›†åˆ
        collections = utility.list_collections(using=conn.alias)
        
        # æ‰¾åˆ°æœ‰æ•°æ®çš„é›†åˆ
        data_collection = None
        for collection_name in collections:
            collection = Collection(name=collection_name, using=conn.alias)
            collection.load()
            if collection.num_entities > 0:
                data_collection = collection_name
                logger.info(f"æ‰¾åˆ°æœ‰æ•°æ®çš„é›†åˆ: {collection_name} (å®ä½“æ•°: {collection.num_entities})")
                break
        
        if not data_collection:
            logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•°æ®çš„é›†åˆ")
            return False
        
        # è§£æé›†åˆåç§°
        parts = data_collection.split('_')
        if len(parts) < 2:
            logger.error(f"é›†åˆåç§°æ ¼å¼ä¸æ­£ç¡®: {data_collection}")
            return False
        
        # å¯¹äº ragflow_ å¼€å¤´çš„é›†åˆï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        if data_collection.startswith('ragflow_'):
            # ragflow_7179adc24c1b11f0bb2a6b89a3fc27c2_7b5bf8b84c1b11f0bb2a6b89a3fc27c2
            parts = data_collection.split('_')
            if len(parts) >= 3:
                index_name = '_'.join(parts[:-1])  # ragflow_7179adc24c1b11f0bb2a6b89a3fc27c2
                kb_id = parts[-1]  # 7b5bf8b84c1b11f0bb2a6b89a3fc27c2
            else:
                index_name = '_'.join(parts[:-1])
                kb_id = parts[-1]
        else:
            index_name = '_'.join(parts[:-1])
            kb_id = parts[-1]
        
        logger.info(f"è§£æç»“æœ - ç´¢å¼•å: {index_name}, çŸ¥è¯†åº“ID: {kb_id}")
        
        # è·å–é›†åˆè¯¦ç»†ä¿¡æ¯
        collection = Collection(name=data_collection, using=conn.alias)
        collection.load()
        
        # æ‰¾åˆ°å‘é‡å­—æ®µ
        vector_fields = []
        for field in collection.schema.fields:
            if 'VECTOR' in str(field.dtype):
                dim = field.params.get('dim', 0) if hasattr(field, 'params') else 0
                vector_fields.append({
                    'name': field.name,
                    'dim': dim
                })
        
        logger.info(f"æ‰¾åˆ°å‘é‡å­—æ®µ: {vector_fields}")
        
        if not vector_fields:
            logger.error("é›†åˆä¸­æ²¡æœ‰æ‰¾åˆ°å‘é‡å­—æ®µ")
            return False
        
        # æµ‹è¯•æ¯ä¸ªå‘é‡å­—æ®µ
        for vector_field in vector_fields:
            field_name = vector_field['name']
            dim = vector_field['dim']
            
            logger.info(f"\næµ‹è¯•å‘é‡å­—æ®µ: {field_name} (ç»´åº¦: {dim})")
            
            try:
                # åˆ›å»ºæµ‹è¯•å‘é‡
                test_vector = [0.1 * i for i in range(dim)]
                
                # åˆ›å»ºåŒ¹é…è¡¨è¾¾å¼
                match_expr = MatchDenseExpr(
                    vector_column_name=field_name,
                    embedding_data=test_vector,
                    embedding_data_type="float",
                    distance_type="L2",
                    topn=10
                )
                
                # æ‰§è¡Œæœç´¢
                logger.info("æ‰§è¡Œæœç´¢...")
                results = conn.search(
                    selectFields=["id", "doc_id", "kb_id", "content_ltks"],
                    highlightFields=[],
                    condition={},
                    matchExprs=[match_expr],
                    orderBy=OrderByExpr(),
                    offset=0,
                    limit=5,
                    indexNames=[index_name],
                    knowledgebaseIds=[kb_id]
                )
                
                # æ£€æŸ¥ç»“æœ
                total = conn.getTotal(results)
                chunk_ids = conn.getChunkIds(results)
                
                logger.info(f"æœç´¢ç»“æœ:")
                logger.info(f"  - æ€»æ•°: {total}")
                logger.info(f"  - è¿”å›æ¡ç›®: {len(chunk_ids)}")
                logger.info(f"  - ç»“æœç±»å‹: {type(results)}")
                
                if total > 0:
                    logger.info(f"  - ç¤ºä¾‹ID: {chunk_ids[:3]}")
                    logger.info(f"  âœ“ æœç´¢æˆåŠŸ! æ‰¾åˆ° {total} æ¡ç»“æœ")
                    
                    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
                    if isinstance(results, dict) and "hits" in results:
                        hits_data = results["hits"]
                        if isinstance(hits_data, dict) and "hits" in hits_data:
                            for i, hit in enumerate(hits_data["hits"][:3]):
                                logger.info(f"    ç»“æœ {i+1}:")
                                logger.info(f"      ID: {hit.get('_id', 'N/A')}")
                                logger.info(f"      Score: {hit.get('_score', 'N/A')}")
                                source = hit.get('_source', {})
                                logger.info(f"      Doc ID: {source.get('doc_id', 'N/A')}")
                                logger.info(f"      KB ID: {source.get('kb_id', 'N/A')}")
                                content = source.get('content_ltks', '')
                                if content and len(content) > 100:
                                    content = content[:100] + "..."
                                logger.info(f"      Content: {content}")
                    
                    return True
                else:
                    logger.warning(f"  æœç´¢æ— ç»“æœ")
                    
            except Exception as e:
                logger.error(f"  æœç´¢å¤±è´¥: {e}")
                import traceback
                logger.error(traceback.format_exc())
        
        return False
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def test_direct_milvus_search():
    """ç›´æ¥ä½¿ç”¨ Milvus API æµ‹è¯•æœç´¢"""
    try:
        logger.info("\n=== ç›´æ¥ Milvus API æœç´¢æµ‹è¯• ===")
        
        from pymilvus import connections, utility, Collection
        
        # è¿æ¥ Milvus
        connections.connect(alias="direct_test", host="localhost", port="19530")
        
        # æ‰¾åˆ°æœ‰æ•°æ®çš„é›†åˆ
        collections_list = utility.list_collections(using="direct_test")
        data_collection_name = None
        
        for collection_name in collections_list:
            collection = Collection(name=collection_name, using="direct_test")
            collection.load()
            if collection.num_entities > 0:
                data_collection_name = collection_name
                break
        
        if not data_collection_name:
            logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•°æ®çš„é›†åˆ")
            return False
        
        logger.info(f"ä½¿ç”¨é›†åˆ: {data_collection_name}")
        
        collection = Collection(name=data_collection_name, using="direct_test")
        collection.load()
        
        # æ‰¾åˆ°å‘é‡å­—æ®µ
        vector_field = None
        for field in collection.schema.fields:
            if 'VECTOR' in str(field.dtype):
                vector_field = field
                break
        
        if not vector_field:
            logger.error("æ²¡æœ‰æ‰¾åˆ°å‘é‡å­—æ®µ")
            return False
        
        dim = vector_field.params.get('dim', 1024) if hasattr(vector_field, 'params') else 1024
        logger.info(f"å‘é‡å­—æ®µ: {vector_field.name}, ç»´åº¦: {dim}")
        
        # åˆ›å»ºæµ‹è¯•å‘é‡
        test_vector = [0.1 * i for i in range(dim)]
        
        # æ‰§è¡Œæœç´¢
        logger.info("æ‰§è¡Œç›´æ¥æœç´¢...")
        search_results = collection.search(
            data=[test_vector],
            anns_field=vector_field.name,
            param={"metric_type": "L2", "params": {"nprobe": 10}},
            limit=5,
            output_fields=["id", "doc_id", "kb_id"]
        )
        
        if search_results and len(search_results) > 0:
            hits = search_results[0]
            logger.info(f"ç›´æ¥æœç´¢ç»“æœ: {len(hits)} æ¡")
            for i, hit in enumerate(hits[:3]):
                logger.info(f"  ç»“æœ {i+1}: è·ç¦»={hit.distance}, ID={hit.id}")
                entity_dict = hit.entity.to_dict()
                logger.info(f"    æ•°æ®: {entity_dict}")
            return True
        else:
            logger.warning("ç›´æ¥æœç´¢æ— ç»“æœ")
            return False
            
    except Exception as e:
        logger.error(f"ç›´æ¥æœç´¢æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹é’ˆå¯¹æœ‰æ•°æ®é›†åˆçš„ Milvus æœç´¢æµ‹è¯•...")
    
    # 1. æµ‹è¯•æœ‰æ•°æ®çš„é›†åˆ
    ragflow_success = test_with_data_collection()
    
    # 2. ç›´æ¥ Milvus API æµ‹è¯•
    direct_success = test_direct_milvus_search()
    
    if ragflow_success:
        logger.info("\nğŸ‰ RAGFlow æœç´¢æµ‹è¯•æˆåŠŸ!")
    else:
        logger.error("\nâŒ RAGFlow æœç´¢æµ‹è¯•å¤±è´¥")
    
    if direct_success:
        logger.info("ğŸ‰ ç›´æ¥ Milvus API æœç´¢æµ‹è¯•æˆåŠŸ!")
    else:
        logger.error("âŒ ç›´æ¥ Milvus API æœç´¢æµ‹è¯•å¤±è´¥")
    
    logger.info("æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main()
